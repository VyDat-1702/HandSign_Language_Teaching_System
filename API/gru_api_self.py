from fastapi import FastAPI, Request, WebSocket, WebSocketDisconnect
from websockets.exceptions import ConnectionClosed
from fastapi.templating import Jinja2Templates
import uvicorn
import asyncio
import cv2
import torch
from ultralytics import YOLO
import numpy as np
from decimal import Decimal
from collections import deque
import mediapipe as mp  
import time
from mp import *
import os
os.environ["OMP_NUM_THREADS"] = "4"
# Kh·ªüi t·∫°o FastAPI
app = FastAPI()
cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)

# T·∫£i model YOLO ch·ªâ m·ªôt l·∫ßn
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
#khai b√°o c√°c classes
class_names = np.array( ["aw","ee","oo","sac", "hoi","nang","nothing","aa","ow","uw", "nga","huyen"])

# log_dir = os.path.join('Logs_file\Logs_8_5\Logs_GRU_8_5')
# tb_callback = TensorBoard(log_dir=log_dir)
# c·∫•u tr√∫c model LSTM
'''
STEP 3: CREATE MODEL CLASS
'''
import torch
import torch.nn as nn

class GRUModel(nn.Module):
    def __init__(self, input_size, hidden_size1, hidden_size2, num_classes):
        super(GRUModel, self).__init__()
        
        # GRU layer 1
        self.gru1 = nn.GRU(input_size=input_size, hidden_size=hidden_size1, 
                          batch_first=True)
        self.dropout1 = nn.Dropout(p=0.2)
        
        # GRU layer 2
        self.gru2 = nn.GRU(input_size=hidden_size1, hidden_size=hidden_size2, 
                          batch_first=True)
        self.dropout2 = nn.Dropout(p=0.2)
        
        # Flatten (handled in forward)
        # Dense layer
        self.fc = nn.Linear(hidden_size2 * 30, num_classes)  # 30 l√† seq_len
        
        # Softmax kh√¥ng c·∫ßn khai b√°o ri√™ng v√¨ th∆∞·ªùng x·ª≠ l√Ω trong loss function
        
    def forward(self, x):
        # GRU layer 1
        out, _ = self.gru1(x)  # out: [batch_size, seq_len, hidden_size1]
        out = self.dropout1(out)
        
        # GRU layer 2
        out, _ = self.gru2(out)  # out: [batch_size, seq_len, hidden_size2]
        out = self.dropout2(out)
        
        # Flatten
        out = out.reshape(out.size(0), -1)  # [batch_size, seq_len * hidden_size2]
        
        # Dense layer
        out = self.fc(out)  # [batch_size, num_classes]
        
        return out  # Softmax s·∫Ω ƒë∆∞·ª£c √°p d·ª•ng trong loss (CrossEntropyLoss)
# Kh·ªüi t·∫°o m√¥ h√¨nh
input_size = 63    # S·ªë ƒë·∫∑c tr∆∞ng t·∫°i m·ªói timestep
hidden_size1 = 256 # K√≠ch th∆∞·ªõc hidden state c·ªßa GRU ƒë·∫ßu ti√™n
hidden_size2 = 128 # K√≠ch th∆∞·ªõc hidden state c·ªßa GRU th·ª© hai
num_classes = class_names.shape[0]  # S·ªë l·ªõp ƒë·∫ßu ra (t·ª´ bi·∫øn actions_train)

model = GRUModel(input_size, hidden_size1, hidden_size2, num_classes)  
#compile model 
# max_epoch = 120
# LR = 0.001
# criterion = nn.CrossEntropyLoss()
# optimizer = torch.optim.Adam(model.parameters(), lr=LR)
torch.manual_seed(1)
'''
STEP 4: INSTANTIATE MODEL CLASS
'''




#######################
#  USE GPU FOR MODEL  #
#######################
    
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
model.to(device)

# Load tr·ªçng s·ªë m√¥ h√¨nh t·ª´ file ƒë√£ l∆∞u
model.load_state_dict(torch.load('D:/install/AI/Code/Project/GRU_MODEL/gru_model.pth'))

# ƒê·∫∑t m√¥ h√¨nh v√†o ch·∫ø ƒë·ªô ƒë√°nh gi√° (evaluation mode)
model.eval()

# Th√¥ng s·ªë ƒë·∫ßu v√†o
SEQUENCE_LENGTH = 30
PREDICTION_THRESHOLD = 0.5  # Ng∆∞·ª°ng d·ª± ƒëo√°n 50%


templates = Jinja2Templates(directory="templates")

@app.get('/')
def index(request: Request):
    return templates.TemplateResponse("index.html", {"request": request})

# H√†m ƒë·ªçc camera async
async def read_camera():
    loop = asyncio.get_event_loop()
    return await loop.run_in_executor(None, cap.read)

@app.websocket("/ws")
async def get_stream(websocket: WebSocket):
    await websocket.accept()
    try:
        while True:
            success, frame = await read_camera()
            if not success:
                break
                        # H√†ng ƒë·ª£i l∆∞u tr·ªØ frames
            frames_queue = deque(maxlen=SEQUENCE_LENGTH)

            # Tr·∫°ng th√°i c·ªßa ch∆∞∆°ng tr√¨nh
            state = "start"
            start_time = time.time()
            predicted_class = ""
            prediction_score = 0.0
            with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:
                while cap.isOpened():
                    ret, frame = cap.read()
                    if not ret:
                        print("‚ùå Kh√¥ng th·ªÉ ƒë·ªçc frame t·ª´ webcam!")
                        break

                    current_time = time.time()

                    if state == "start":
                        # üü¢ Hi·ªÉn th·ªã th√¥ng b√°o thu th·∫≠p trong 2 gi√¢y
                        if current_time - start_time < 2:
                            cv2.putText(frame, "STARTING COLLECTION", (50, 100),
                                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 3, cv2.LINE_AA)
                        else:
                            state = "collect"
                            frames_queue.clear()  # X√≥a d·ªØ li·ªáu c≈©

                    elif state == "collect":
                        # üü° Thu th·∫≠p SEQUENCE_LENGTH frames
                    
                        frame, results = mediapipe_detection(frame, holistic)

                        # Draw landmarks
                        draw_styled_landmarks(frame, results)

                        # 2. Prediction logic
                        keypoints = extract_keypoints(results)
                        frames_queue.append(keypoints)
                        # count.append(keypoints)
                        # frames_queue.append(frame_resized)

                        cv2.putText(frame, f"Collecting: {len(frames_queue)}/{SEQUENCE_LENGTH}", (30, 50),
                                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)

                        if len(frames_queue) == SEQUENCE_LENGTH:
                            state = "predict"
                            print(len(frames_queue))

                    elif state == "predict":
                        # üî¥ D·ª± ƒëo√°n
                        sequence_tensor = np.expand_dims(frames_queue, axis=0)  # Th√™m chi·ªÅu batch
                        sequence_tensor = torch.tensor(sequence_tensor, dtype=torch.float32).to(device)
                        with torch.no_grad():
                # D·ª± ƒëo√°n
                                    res_tensor = model(sequence_tensor)  # G·ªçi m√¥ h√¨nh tr·ª±c ti·∫øp
                                    res = torch.softmax(res_tensor[0], dim=-1).cpu().numpy()  # Chuy·ªÉn logits th√†nh x√°c su·∫•t
                                    predicted_index = np.argmax(res)
                                    prediction_score = res[predicted_index]
                                    print(prediction_score)

                        if prediction_score >= PREDICTION_THRESHOLD:
                            predicted_class = class_names[predicted_index] if predicted_index < len(class_names) else "Kh√¥ng x√°c ƒë·ªãnh"
                        else:
                            predicted_class = "Kh√¥ng x√°c ƒë·ªãnh"

                        start_time = current_time  # C·∫≠p nh·∫≠t th·ªùi gian ƒë·ªÉ hi·ªÉn th·ªã k·∫øt qu·∫£
                        state = "show_result"

                    elif state == "show_result":
                        # üü† Hi·ªÉn th·ªã k·∫øt qu·∫£ trong 2 gi√¢y n·∫øu ƒë·ªô tin c·∫≠y >= ng∆∞·ª°ng
                        if prediction_score >= PREDICTION_THRESHOLD:
                            cv2.putText(frame, f"Class: {predicted_class}", (30, 50),
                                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)
                            cv2.putText(frame, f"Score: {round(Decimal(prediction_score * 100), 2)}%", (30, 100),
                                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)
                        else:
                            cv2.putText(frame, "Prediction confidence below threshold", (30, 50),
                                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)

                        if current_time - start_time >= 2:
                            state = "start"  # Quay l·∫°i thu th·∫≠p
                    # Encode ·∫£nh v√† g·ª≠i qua WebSocket
                    _, buffer = cv2.imencode('.jpg', frame)
                    await websocket.send_bytes(buffer.tobytes())

                    # Gi·∫£m t·∫£i CPU/GPU
                    await asyncio.sleep(0.01)  # C√≥ th·ªÉ ch·ªânh 0.01 - 0.05 t√πy t·ªëc ƒë·ªô c·∫ßn thi·∫øt

    except (WebSocketDisconnect, ConnectionClosed):
        print("Client disconnected")

if __name__ == '__main__':
    uvicorn.run(app, host='0.0.0.0', port=8001)
